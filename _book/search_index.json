[["index.html", "Data Science In RStudio Chapter 1 Index 1.1 Things to add 1.2 Documentation for Author 1.3 Andrews Notes / Ramblings 1.4 To Do 1.5 Title word cloud 1.6 Copy from Chapter 1 Example 1.7 MATLAB Highlighting 1.8 Misc code/data 1.9 Other Resources", " Data Science In RStudio Andrew P. Lapointe 2020-11-03 Chapter 1 Index 1.1 Things to add Signe recommended students run a live tutorial in swirl 1.2 Documentation for Author 1.3 Andrews Notes / Ramblings To render the book used the following code, you must do this before knitting the GitBook (webpage) bookdown::render_book(&quot;index.Rmd&quot;, &quot;bookdown::gitbook&quot;) bookdown::render_book(&quot;index.Rmd&quot;, &quot;bookdown::pdf_book&quot;, encoding=&quot;UTF-8&quot;) bookdown::render_book(&quot;index.Rmd&quot;, &quot;bookdown::epub_book&quot;) The _output.yml contains the header arguments. I would but them here so its cleaner and easier to read the code. 1.4 To Do Is there a way I can have matlab code syntax highlighted properly? Maybe here Get Camera to take nice pictures Get example dataset that I can run through Can I export this to github.io? The GitHub Repository for this guidebook can be found here Add highlight arg to _output.yml Upload the MATLAB packages required to GitHub directory Finish my changes Push my commits to GitHub Open a pull request Cleaned the main directory to make it easier to interpret for beginners For Chapter 5 steal logo from here. It would be great to have a logo here that was cowboy themed These animations would be great at explaining some key concepts 1.5 Title word cloud Titles: inquiry-based R for researchers in a hurry Reproducible science 1.6 Copy from Chapter 1 Example You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter \\@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \\@ref(methods). Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure \\@ref(fig:nice-fig). Similarly, you can reference tables generated from knitr::kable(), e.g., see Table \\@ref(tab:nice-tab). knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) You can write citations, too. For example, we are using the bookdown package in this sample book, which was built on top of R Markdown and knitr . 1.7 MATLAB Highlighting % This is a comment in MATLAB function y = average(x) if ~isvector(x) error(&#39;Input must be a vector&#39;) end y = sum(x)/length(x); end So could this 1.8 Misc code/data gitbook(fig_caption = TRUE, number_sections = TRUE, self_contained = FALSE, lib_dir = &quot;libs&quot;, pandoc_args = NULL, ..., template = &quot;default&quot;, split_by = c(&quot;chapter&quot;, &quot;chapter+number&quot;, &quot;section&quot;, &quot;section+number&quot;, &quot;rmd&quot;, &quot;none&quot;), split_bib = TRUE, config = list(), table_css = TRUE) 1.9 Other Resources https://medium.com/@huixiangvoice/the-hidden-story-behind-the-suicide-phd-candidate-huixiang-chen-236cd39f79d3?fbclid=IwAR3zJ0zHAhNaHhRdXkiHfHOJV9RjZWDW6KReN6FB9lkxGeWwzWsDoYYTv10 Book called Just Enough R To test the normality of your data you can use a few different methods http://www.sthda.com/english/wiki/normality-test-in-r https://rstudio-pubs-static.s3.amazonaws.com/2002_1f803b2bc84c46008d3599a07867a95a.html Plot your data Check skewness and kurtosis Shapiro test. There should be a section on general access to your df. How to manipulate it effectively (perhaps in the data wrangling section) This would include things like using df$colname to access a column "],["intro.html", "Chapter 2 Introduction 2.1 Preamble 2.2 How this book is structured 2.3 Keys to Learning 2.4 Quick Introduction to reproducible science 2.5 How to use this book 2.6 Reproducible Science Link 2.7 How to make a provocative Conference Poster 2.8 Misc Student Resources", " Chapter 2 Introduction 2.1 Preamble This documents original purpose was my own cheat sheet to keep notes on data science problems I encountered in R during my work. I transitioned to R from MATLAB after experiencing tremendous frustration making plots for a paper I published. As I continued throughout my PhD, I remember writing 10 pages for my thesis when suddenlymy document froze and I lost all my work. From my mentors at the time I used a very traditional workflow MATLAB was used to process data which was then Imported into SPSS so I could run statistics The output from SPSS was then brought into Excel or MATLAB again so I could make nice figure (SPSS isnt great at this) Finally everything was compiled into a Word (*.docx) document References were handled using EndNote. I remember leaving the lab that day in frustration, knowing that I would have to rewrite all those pages. I vowed to find a better way to do things, and that way for me has been RStudio. Its not perfect, but its the closest thing I have found. This book will bring you through several tasks that an everyday scientist has to accomplish and demonstrates them using YouTube videos and examples. Add figures and content from the Data Battles presentations 2.2 How this book is structured Maybe add description here for this. 2.3 Keys to Learning Do not try to memorize code you can easily look up. In my experience, the best way to get things done is to have a Template with good description. When I start a new project I simply make a copy of it. 2.4 Quick Introduction to reproducible science I am guilty of this in my older projects. Have you ever looked back on data you ran several years ago only to realize you have no idea what data was used to create certain figures or tables? Traditional project pipelines usually involve some combination of Data processing in MATLAB Statistics in SPSS, SAS, GraphPad etc Writing the document in Microsoft Word The issue with this pipeline is shown below. Where we go through a very plausible situation where your supervisor asks for changes to be made in your data. Add figures with Jeff/ Academic pipeline from the Data Battles presentations 2.5 How to use this book For the most part, this book is based on my own learning style. Which emanates from actually using the code. Think of this book as a recipe that you can follow. In most cases you should be able to copy / paste the code chunks and modify them slightly to work with your code. My general advice is to get used to spotting patterns in code. You may not need to understand every argument within a function. If your particular problem does require more specific code, you can always look up your given function on rdocumentation.org 2.6 Reproducible Science Link This site contains a list of curated sources discussing reproducibility. You can find academic papers, blog posts, popular media articles, talks, tools, and more. List of Resources https://github.com/qinwf/awesome-R#awesome- List of resources from easystats blog 2.7 How to make a provocative Conference Poster This link gives one of the best overviews I have seen into what should be included in an academic poster. Here are a few more links https://odeleongt.github.io/postr/ https://github.com/GerkeLab/betterposter https://wytham.rbind.io/post/making-a-poster-in-r/#fnref1 https://hsp.berkeley.edu/sites/default/files/ScientificPosters.pdf 2.8 Misc Student Resources sci-hub.tw Gets you full-length research articles without the paywall. I recommend integrating it inside Zotero. Link 1 EndNote Click (previously Kopernio) Is an alternative to sci-hub but its directly integrated into your browser (requires institutional login). https://b-ok.cc/ This is similar to sci-hub but it works for books. Not sure where the ethical line falls on this one. You can use it to get pdfs on Books such as Writing your first paper. QuillBot AI Paraphrasing Tool. Can be useful when you have writers block and need some suggestions. Be careful of plagiarism. Corporate BS Generator There are a few of these you can Google. The words they can provide can be good to include in grants to sound fancy. "],["installation.html", "Chapter 3 Installation 3.1 Starting Off 3.2 Setting up RStudio for the first time 3.3 Installing R + RStudio 3.4 Already have R and RStudio installed? 3.5 Update RStudio 3.6 How to use packages 3.7 Alternatives to installing / loading packages 3.8 Trouble installing packages 3.9 Other things to do in RStudio 3.10 Conclusion 3.11 Chapter 1b: Project Environment 3.12 Setting up your work environment", " Chapter 3 Installation 3.1 Starting Off This is, in my opinion, by far the most frustrating part about learning R. Getting setup is a bit of a pain. But once you are setup things work very well. I am going to do my best to walk you through all the things you need. This should take you about 2 hours to complete. It sucks, but just do it and then were off to the races. Theres some documentation over on my GitHub which should help a bit. 3.2 Setting up RStudio for the first time 3.3 Installing R + RStudio This might be the most frustrating part. It can be a bit tricky and takes time to setup. It should take about an hour to accomplish (assuming you have decent internet speeds) 3.4 Already have R and RStudio installed? To check which version of R you are running simply type version in the Console knitr::include_graphics(&quot;https://i.imgur.com/qH9RNAf.png&quot;) Figure 3.1: Check your R version To check your version of RStudio knitr::include_graphics(&quot;https://i.imgur.com/kybzMim.png&quot;) Figure 3.2: Check your RStudio version knitr::include_graphics(&quot;https://i.imgur.com/qJ4eR8r.png&quot;) Figure 3.3: Check your RStudio version: Part II Another method is to use the rstudioapi package which allows you to do this via the Console 3.4.1 How to update R With some packages it is possible that you will need to update R and/or RStudio (notice that these are different). RStudio is the GUI which runs R. If you are using Windows I would highly recommend using the installr package. if (!require(&#39;remotes&#39;)) install.packages(&#39;remotes&#39;); # make sure you have Rtools installed first! if not, then run: #install.packages(&#39;installr&#39;) #install.Rtools() remotes::install_github(&#39;talgalili/installr&#39;) Launch RGui and then run the following code if(!require(&quot;installr&quot;)) install.packages(&#39;installr&#39;) library(&quot;installr&quot;) updateR() # this will open dialog boxes to take you through the steps. There is apparently a version in development for this to work with OSX install.packages(&#39;devtools&#39;) #assuming it is not already installed library(devtools) install_github(&#39;andreacirilloac/updateR&#39;) library(updateR) updateR(admin_password = &#39;Admin user password&#39;) 3.5 Update RStudio I would highly recommend trying RStudio 1.4. It is currently a preview release. I have been using it for a few months now and some of the new additions are amazing You can download it here ## Other things to install tiny_tex Biocmanager List of potential packages 3.6 How to use packages packages include a list of functions that can be used in your code. One of the most widely used packages is the tidyverse. In order to use the package we would normally use the library(packageName)which in this case would be library(tidyverse). However, before you can load a library for the first time it must be installed on your computer. Packages are traditionally downloaded from CRAN, which is an RStudio curated list of packages. Packages on CRAN are tested for stability. You can also download packages directly from GitHub. Packages from GitHub can be (but are rarely) unstable. They can be packages created by anyone, which is one of the main strengths of R. 3.6.1 Installing packages on your computer This only needs to be done ONCE. Install from CRAN you would use the following code install.packages(&quot;package1&quot;, dependencies = TRUE) # This will install &#39;package1&#39; on your computer install.packages(&quot;package2&quot;, dependencies = TRUE) # This will install &#39;package1&#39; on your computer To install from GitHub the devtools package is required. Here is a GitHub package called easystats which can be found here on GitHub. install.packages(&quot;devtools&quot;) # This will install devtools on your computer library(devtools) # load the &#39;devtools&#39; library devtools::install_github(&quot;easystats/easystats&quot;) # install a package off GitHub using &#39;devtools&#39; 3.7 Alternatives to installing / loading packages Personally, I find that this approach to installing/loading packages is quite cumbersome. If you have 5 packages here is what your script would look like install.packages(&quot;package1&quot;, dependencies = TRUE) # This will install &#39;package1&#39; on your computer install.packages(&quot;package2&quot;, dependencies = TRUE) # This will install &#39;package2&#39; on your computer install.packages(&quot;package3&quot;, dependencies = TRUE) # This will install &#39;package3&#39; on your computer install.packages(&quot;package4&quot;, dependencies = TRUE) # This will install &#39;package4&#39; on your computer install.packages(&quot;package5&quot;, dependencies = TRUE) # This will install &#39;package5&#39; on your computer library(package1) # This will load &#39;package1&#39; in your workspace library(package2) # This will load &#39;package2&#39; in your workspace library(package3) # This will load &#39;package3&#39; in your workspace library(package4) # This will load &#39;package4&#39; in your workspace library(package5) # This will load &#39;package5&#39; in your workspace An alternative to this would be use the pacman package which I fell in love with when I was learning R. Here is the code I have at the top of my scripts. p_load This function is a wrapper for library and require. It checks to see if a package is installed, if not it attempts to install the package from CRAN and/or any other repository in the pacman repository list. For a list of pacman functions here: knitr::include_graphics(&quot;https://i.imgur.com/T8xl8Bi.png&quot;) Figure 3.4: List of pacman functions if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load(package1, package2, package3, package4, package5) #p_load - This function is a wrapper for library and require. It checks to see if a package is installed, if not it attempts to install the package from CRAN and/or any other repository in the pacman repository list. if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) # This will install/load the &#39;pacman&#39; package pacman::p_load(package1, package2) # This will check to see if &#39;package1&#39; and &#39;package2&#39; is installed on your computer, otherwise it will install it and then load it. 3.8 Trouble installing packages At times you can have issues installing new packages, this can be for a variety of reasons. Try closing and re-opening RStudio. At times packages can conflict with each other. Some packages depend on other packages to work. For example the psych package which requires or depends on the mnormt package. My first test is to unload package which can be done using the code below pacman::p_unload(all) # detaches all packages. Very useful when you need to update a package OR install a packages which has dependencies that are already loaded in your R Session You are using a very old version of R This is rather unlikely, but make sure you are using at least version 4.0 or newer. If you have conflicting packages see here conflicted::conflict_prefer(&quot;tidy&quot;, &quot;broom&quot;) # for a given function e.g., tidy(lm) it will prefer the function from tidy before broom 3.9 Other things to do in RStudio 3.9.1 Themes: Bring on the darkness I typically do a few other small things to setup. The first is I install a dark theme. There are plenty of default themes available but I prefer using one from GitHub called rscodeio. Its not perfect but its my personal favorite at the moment. I change the color of the cursor to a bright pink as well as the color of syntax highlighting. remotes::install_github(&quot;anthonynorth/rscodeio&quot;) # install the package rscodeio::install_theme() # install the theme 3.9.2 Shortcuts Ctrl+Alt+L for Clear Workspace this is a shortcut I use a lot to reset my environment 3.9.3 Code Folding Insert an animation showing how to fold code. Code sections allow you to break a larger source file into a set of discrete regions for easy navigation between them. Code sections are automatically foldablefor example, the following source file has three sections (one expanded and the other two folded): To insert a new code section you can use the Code -&gt; Insert Section command. Alternatively, any comment line which includes at least four trailing dashes (-), equal signs (=), or pound signs (#) automatically creates a code section. For example, all of the following lines create code sections: # Section One --------------------------------- # Section Two ================================= ### Section Three ############################# Theres a couple more examples on StackOverflow here Note that as illustrated above the line can start with any number of pound signs (#) so long as it ends with four or more -, =, or # characters. # SECTION1 ---- # . Subsection1.1 ---- 3.10 Conclusion Now that you have your environment setup, we can finally start with some coding. I would suggest you take a look at my optional chapter on Project Environment where we break down where things should be stored in a typical research project. This will come into play if you decide to create a reproducible document using RMarkdown in later chapters. Its also just a good idea to keep things organized so you can find stuff within your project. 3.11 Chapter 1b: Project Environment Here I give a quick breakdown of a typical project directory. 3.12 Setting up your work environment 3.12.1 Folder Organization Check a few of the websites I have bookmarked on this subject knitr::include_graphics(&quot;images/FolderStructure.png&quot;) Here is an example from a project where folder structures were not followed. It is an unmitigated disaster. https://chrisvoncsefalvay.com/2018/08/09/structuring-r-projects/ https://www.thinkingondata.com/how-to-organize-data-science-projects/ https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000424 https://drivendata.github.io/cookiecutter-data-science/#opinions http://www.theexclusive.org/2012/08/principles-of-research-code.html https://martinctc.github.io/blog/rstudio-projects-and-working-directories-a-beginner's-guide/ "],["first-script.html", "Chapter 4 Creating your first script 4.1 Part 1: Loading/installing packages needed 4.2 Part 2: Loading your data 4.3 Cleaning your imported data. 4.4 Part 3: Saving Outputs", " Chapter 4 Creating your first script A script can be broken down into a few layers which we are going to go through in detail, but heres a general overview. Load/install required packages Load your data Perform data wrangling tasks Then what? create figures run statistics create tables Save outputs Figures are saved in your /images directory, whereas the statistics and tables are saved in /data for future use. Each section is coded so that I can fold/unfold a given section. This allows me to only focus on the section of code that is important at that given point and time. 4.1 Part 1: Loading/installing packages needed At the very top of my script I will load the packages I need to get things done. This will vary slightly depending on the script. For example some of my packages (e.g., lme4 or easystats) are only loaded when I run statistics. Theres a few example below, I suggest skipping to the section that applies to your situation. # Load/Install required packages --------------------- if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load(conflicted,readxl, ggplot2, esquisse, Rmisc, tidyverse, car, easystats, apastats, sjlabelled) #p_load This function is a wrapper for library and require. It checks to see if a package is installed, if 4.2 Part 2: Loading your data You should have data that you are trying to manipulate. Below I show the most common examples. I generally always call my data df. This allows me to easily copy/paste code between projects. Its good practice to do this if you can. Therefore, as you advance with your R scripts you wont need to spend precious time using Find/Replace. 4.2.1 xlsx # Load your dataset --------------------- df &lt;- read_excel(&quot;raw/CC_Body_FA.xlsx&quot;, sheet = &quot;Sheet1&quot; ) # import your dataset - uses &#39;readxl&#39; 4.2.2 csv When I have particularly large files to write from MATLAB, I prefer to use *.csv files over *.xlsx because they write faster. If you are dealing with datasets that are larger than 1GB in size you should consider using data.table instead of data.frame. # Load your dataset --------------------- df = data.table::fread(&quot;C:/Users/DunnLab/Google Drive/Aty_fNIRS - Seven Day/raw/TimeSeriesDatasetLobesMean.csv&quot;, header = TRUE) # requires &#39;data.table&#39; library 4.2.3 Google Sheet It is also possible to read from a Google Sheet using the googlesheets4 package. df &lt;- read_sheet(&quot;https://docs.google.com/spreadsheets/d/1V99DMca-Qdy3G7kyg9zTONvBVagtnBrj4nm78Fj1vU8&quot;, sheet = &quot;Head Measures &amp; Information&quot;) # requires &#39;googlesheets4&#39; library 4.2.4 sav (SPSS) This is the general data format for SPSS. You can do so by using the foreign package. A full tutorial on importing other data types can be seen here on DataCamp. In general, try and stick to the formats shown above. Finally, its possible you want to open data from other forms including 1. .txt 2. . 3. SPSS 4. Mini-table 4.3 Cleaning your imported data. Its possible to clean up your dataset as it comes in by using the janitor package. Click the link for a couple examples. In essence it will scan through the column names and fix them according to a notation you specify. Now that you have your df loaded, lets take a look and see what we have. There are 4 types of data that can be held in a data.frame, in R these are referred to as class. Numeric Characters Factors Dates You can view the type within a particular column by running the following code sapply(df, class) The class of your columns may not seem important right now, but later on when we manipulate the data, it will be crucial to make sure these are accurate. Below is an example of an xlsx file which is imported. We expected dti_value to be numeric, but due to a dash in one of the cells, it was imported as character. 4.4 Part 3: Saving Outputs # Save your environment ------------ # Save it to .RData ----------- save(journey_time,modsum, model, file = &quot;data/analyzedData.RData&quot;) #Save a list of tables that I&#39;ll use in the .Rmd file. # Save the tables into data/tables.RData using &quot;patterns&quot; ================== save(list=ls(pattern=&quot;table&quot;), file = &quot;data/tables.RData&quot;) #Save a list of tables that I&#39;ll use in the .Rmd file. save(list=ls(pattern=&quot;mod&quot;), file = &quot;data/stats.RData&quot;) # Optional - Save df as xlsx -------- xlsx::write.xlsx(tmp2, &quot;data/interactions.xlsx&quot;, sheetName = &quot;Interaction2&quot;, append = TRUE) # uses the xlsx package openxlsx::write.xlsx(daily, &quot;data/daily.xlsx&quot;) # uses the openxlsx package but you can&#39;t append sheets with this package as far as I know. "],["datawrangling.html", "Chapter 5 Data Wrangling 5.1 Intro 5.2 Exploratory Data Analysis 5.3 Creating Report with DataExplorer 5.4 Tasks 5.5 For Item-based data", " Chapter 5 Data Wrangling 5.1 Intro Data Wrangling is the process of cleaning and manipulating your data.frame. I have never obtained a dataset that did not need to be cleaned or organized in some fashion (if you have I am quite jealous). The DataExplorer package allows you to get a preliminary look at your data. It will check for missing data 5.2 Exploratory Data Analysis Phase I of my statistics is usually termed Data Exploration or Exploratory Data Analysis The goal of this step is to gain valuable insights through the data so that one can know what is going on with the data, which part needs to be cleaned, what new features can be built, build hypotheses to be tested during the model creation/validation phase, or even just knowing some fun facts about the data (src). My 2 favorite packages to get a glimpse of the data are SmartEDA DataExplorer summarytools dataMaid janitor and here 5.3 Creating Report with DataExplorer create_report( df.fa, # the name of your dataframe #y = &#39;heart_disease&#39;, output_dir = &#39;output&#39;, # where do you want it to be saved relative to your project directory output_file = &#39;data_explorer_fa_report.html&#39;, # the filename for the report report_title = &#39;DTI (FA) Data Description&#39; # the Title of your report ) SmartEDA::ExpNumStat(tbl.desc, round = 1) ExpNumStat( tbl.desc, by = &quot;GA&quot;, gp = &quot;Group&quot;, Qnt = c(.1, .9), Outlier = TRUE, round = 1 ) ExpNumViz(tbl.desc, target = &#39;Group&#39;) summarytools::dfSummary( tbl.desc, varnumbers = FALSE, round.digits = 2, plain.ascii = FALSE, style = &quot;grid&quot;, graph.magnif = .33, valid.col = FALSE, tmp.img.dir = &quot;img&quot; ) 5.3.1 Reading Material : Exploratory Data Analysis https://m-clark.github.io/exploratory-data-analysis-tools/ https://blog.revolutionanalytics.com/2018/02/dataexplorer.html https://www.avery-robbins.com/2020/06/01/first-post-ever/ 5.4 Tasks 5.4.1 Inspecting the imported data Among the issues you may encounter with your data is inconsistencies in columnnames. This can be rather annoying. In order to alleviate this, I like to pick one format and stick with it. When I receive a dataset from another researcher I will often use snakecase to modify their dataset accordingly. 5.4.2 Choosing good column names Avoid using spaces. This makes it more tedious to refer or call a column later on. In the example below our task is to rename the column Group Level. It should also be noted that things can get tricky when you are using certain packages and will over time, be a large burden to you. Hence, my advice is to remove spaces. Consider going into snakecase. df$`Group Level` # Column name with spaces df$GroupLevel # Column name without spaces 5.4.3 replace strings using pipes In the code below I want to replace text in the Test column. Notice how I call the column once and then use the pipe operator to list the rest df %&gt;% mutate(Test = str_replace_all(Test, &quot;:&quot;, &quot;×&quot;) %&gt;% str_replace_all(&quot;group2.L&quot;, &quot;Group&quot;) %&gt;% str_replace_all(&quot;mriloc2&quot;, &quot;&quot;) %&gt;% str_replace_all(&quot;hemisphereLeft&quot;, &quot;LeftHemi&quot;) %&gt;% str_replace_all(&quot;hemisphereRight&quot;, &quot;RightHemi&quot;) ) 5.4.4 Useful Links Below is a list of pages I have collected that have come in very useful for beginners 5.4.4.1 Selecting using the tidyverse https://suzan.rbind.io/2018/02/dplyr-tutorial-3/#filtering-based-on-a-exact-character-variable-matches 5.4.4.2 Working with dates We wont cover working with dates much in this guide, because its not typical. However, should you need them the link below provides a general walkthrough. https://www.r-bloggers.com/a-comprehensive-introduction-to-handling-date-time-in-r-2/ # Adding Names to Columns ================== # This can be required when using certain plotting functions like &quot;likert&quot; --&gt; see ALPH likert4.R names(items) &lt;- c( symptoms_decLOC=&quot;Did the patient have loss of consciousness?&quot;, symptoms_headache=&quot;Did the patient have headaches?&quot;, symptoms_nausea_vomitting=&quot;Did the patient have nausea/vomiting?&quot;, symptoms_cranialNerve_paresis=&quot;Did the patient have cranial nerve paresis?&quot;, symptoms_gait_disturbance=&quot;Did the patient have any gait disturbances?&quot;) # Data Wrangling ---------------- df1$Group &lt;- rep(&quot;Body&quot;, length(df1$`File name`)) df2$Group &lt;- rep(&quot;Genu&quot;, length(df2$`File name`)) df3$Group &lt;- rep(&quot;Splenium&quot;, length(df3$`File name`)) idx &lt;- unique(df3l$`File name`) == df1$`File name` dfx &lt;- df1 %&gt;% select_if(df1$`File name` == unique(df3$`File name`)) combinedDF &lt;- rbind(df1,df2,df3) df1[!(df1$`File name` %in% df3$`File name`)] idx &lt;- setdiff(df1$`File name`, df2$`File name`) df1 &lt;- df1[,idx] # Convert several columns to numeric ---------- cols &lt;- grep(pattern = &quot;CC_F|Left|Right&quot;, x = names(df), value = TRUE) df[, cols] &lt;- lapply(df[ , cols], function(x) suppressWarnings(as.numeric(x))) #supressWarnings is so you don&#39;t get &quot;NAs introduced by coercion&quot; in your console output # Count the number of NA&#39;s in each column of a dataframe ---------- sapply(df, function(x) sum(is.na(x))) #this is different than &quot;summary(df)&quot; which gives you information on more than NA&#39;s # Add column in specific spot ======== df &lt;- add_column(df, &quot;DMT_line2&quot; = df$DMT_line, .after = &quot;DMT_line&quot;) # Add a blank column in a specific spot ===== items &lt;- add_column(items, &quot;Avg&quot; = replicate(length(items$value.D1), NA) , .after = &quot;value.D7&quot;) # Remove blank rows in a specific column ======= df &lt;- df[-which(df$start_pc == &quot;&quot;), ] # Removing a column based on a string match ========= df &lt;- filter(df, C != &quot;Foo&quot;) # Renaming a variable in a column : str_replace (this won&#39;t take exact strings)============= df1$ID &lt;- df$ID %&gt;% str_replace(&quot;pi6437934_2&quot;, &quot;S2&quot;) # Replace an exact string ------------- data$OpenBCI_FileName &lt;- gsub(&quot;\\\\&lt;y\\\\&gt;&quot;,&quot;&quot;, data$OpenBCI_FileName) # replaces cols with &quot;y&quot; but won&#39;t touch something like &quot;My Drive&quot; # if you have multiple strings to replace you can use the pipe operator to get everything done in one shot df$ID &lt;- df$ID %&gt;% str_replace(&quot;pi_14344894_9&quot;, &quot;S9&quot;) %&gt;% #oldstring, newstring str_replace(&quot;pi_3478_03o4_15&quot;, &quot;S15&quot;) # Partial String match in a column---------- # __Method 1: str_detect ======== mtcars %&gt;% filter(str_detect(rowname, &quot;Merc&quot;)) # this will filter everything that has the pattern &quot;Merc&quot; in the column # __Method 2 : grepl ========== dplyr::filter(mtcars, !grepl(&#39;Toyota|Mazda&#39;, type)) # Using grepl cols &lt;- grep(pattern = &quot;symptoms|Previous_shunt&quot;, x = names(df), value = TRUE) #Select columns that start with symptoms df[cols] &lt;- as.data.frame(lapply(df[cols],function(x) {factor(x,levels=mylevels, exclude=&quot;&quot;)})) #__Method 3 : grep (similar to grepl) ===== cols &lt;- grep(pattern = &quot;symptoms&quot;, x = names(df2), value = TRUE) #Select columns that start with symptoms # Renaming a column ================ df &lt;- df %&gt;% df &lt;- df %&gt;% dplyr::rename( #New Column Name= Old Name &quot;month&quot;= &quot;Month&quot; ) # Removing NA&#39;s----------- # __Method 1: is.na() ==== df &lt;- df %&gt;% filter(!is.na(columnName)) # give me a new dataframe with only the international services. # __Method 2: na.omit() ==== df1 &lt;- na.omit(df1) # Filtering our data that fits a condition: filter========= new_df &lt;- filter(df, service==&quot;International&quot;) # give me a new dataframe with only the international services. To rename parts of a column using %&gt;% you need to use mutate # normal way to replace strings in a column df$columnName = str_replace_all(df$columnName, &quot;Contribution_&quot;, &quot;&quot;) # Using pipe df %&gt;% mutate(columnName = str_replace_all(columnName, &quot;Contribution_&quot;, &quot;&quot;)) 5.5 For Item-based data The likert package seems great. I have used it to great success in past projects. "],["creating-tables.html", "Chapter 6 Creating tables", " Chapter 6 Creating tables In our final document we will want tables to describe our data. Below we create a table which contains demographic information on our sample. # Taking the mean and confidence interval ============= # __Method #1 - Easy ######### journey_time &lt;- Rmisc::summarySE(data = df, measurevar = &quot;journey_time_avg&quot;, groupvars = c(&quot;service&quot;,&quot;year&quot;), conf.interval = 0.95, na.rm = TRUE, .drop = TRUE) # To get a descriptive table tbl.descFullACAP &lt;- Rmisc::summarySE(data = df.mri %&gt;% filter(metric == &quot;FA&quot;, mriloc == &quot;CC_FMajor&quot;), measurevar = &quot;age&quot;, groupvars = c(&quot;group&quot;,&quot;gender&quot;), conf.interval = 0.95, na.rm = TRUE, .drop = TRUE) %&gt;% janitor::clean_names(&quot;upper_camel&quot;) %&gt;% rename( &quot;CI&quot; = &quot;Ci&quot;, &quot;SE&quot; = &quot;Se&quot;, &quot;SD&quot; = &quot;Sd&quot; ) %&gt;% mutate_at(vars(Age,SD,SE,CI), funs(round(., 3))) cgwtools::resave(tbl.descFullACAP, file = &quot;data/tables.RData&quot;) #resave a list of tables that I&#39;ll use in the .Rmd file. # __Method 2 - More options df %&gt;% group_by(Channel) %&gt;% summarise_each(funs(mean, sd)) dt &lt;- data.table(df) group_by(dt, Channel_46) # __Method 2b - Not so Easy ################## # descriptives &lt;- demo%&gt;%dplyr::group_by(AthleteType)%&gt;% # dplyr::summarise( # &quot;Mean Height (cm)&quot; = round(mean(Height_cm),2) # , &quot;Mean Weight (kg)&quot; = round(mean(Weight_kg),2) # , &quot;Mean Age (Years)&quot; = round(mean(AgeInYears),2) # , &quot;Number of Concussions&quot; = round(mean(NumPriorConc),2) # , &quot;Number of Diagnosed Concussions&quot; = round(mean(NumDiagConc),2) # , &quot;Number of Undiagnosed Concussions&quot; = round(mean(NumUndiagConc),2) # ) You may also choose to write these to an xlsx file df &lt;- iris xlsx::write.xlsx2(df, &quot;test.xlsx&quot;, row.names = FALSE, sheetName = &quot;SheetNumber1&quot;, append = TRUE) # uses the xlsx package "],["plotting-using-ggplot.html", "Chapter 7 Plotting using ggplot", " Chapter 7 Plotting using ggplot Insert the paper comparing grammar of graphics vs MATLAB You can look at this page to create weird data When inserting images into your RMarkdown document you can use the (Aspect Ratio Calculator] This chapter could be a book on its own, but we are going to go over a few different principles. First without many exceptions, we use ggplot2 to create plots in R. There are a few other packages that depend on ggplot2 which I use to model statistical data, but we will get to those at a later time. For inspiration here are a couple of sites you can browse through R-Graph Gallery Top 50 ggplot Visualizations ggforce might be a good package to use for my 10-20 figures Figure 7.1: My experience with MATLAB plots # Plotting ------------- journey_time$year &lt;- as.factor(journey_time$year) ggplot(data = df, aes(x= ..., y = ..., color = , )) plot1 &lt;- ggplot(journey_time) + aes(x = service, fill = year, weight = journey_time_avg) + geom_bar(position = &quot;dodge&quot;) + scale_fill_hue() + labs(x = &quot;Service&quot;, y = &quot;Journey Time Average&quot;, title = &quot;My Plot Name&quot;, fill = &quot;Year&quot;) + theme_minimal() # Save the plot we created in &quot;/images&quot; folder ==================== ggsave(&quot;images/plot1.png&quot;,plot1, width=11, height=8.5, dpi=300) If you find yourself using the same attributes over and over again you can save them in a list beforehand and then call them gglayers &lt;- list( geom_boxplot() , geom_point(size = 15, aes(shape = Gender, color = group2)) , scale_shape_manual(values=c(&quot;&quot;,&quot;&quot;), name = &quot;Sex&quot;) , # I need 9 values (I for each ID) scale_color_manual(values=c(&#39;springgreen4&#39;, &#39;red4&#39;), name = &quot;Group&quot;, labels = c(&quot;Orthopedic-injured&quot;, &quot;Brain-injured&quot;)) , scale_fill_manual(values =c(&#39;springgreen4&#39;, &#39;red4&#39;), guide = F) , #facet_wrap(~hemisphere), theme_minimal() , theme(legend.position = &quot;top&quot;, legend.title = element_text(size = 12), plot.title = element_text(hjust = 0.5), plot.caption = element_text(face = &quot;italic&quot;), #legend.key = element_rect(colour = &#39;white&#39;, fill = &#39;white&#39;, size = 0.5, linetype=&#39;dashed&#39;), #legend.key.size = unit(2, &quot;cm&quot;), #legend.key.width = unit(2, &quot;cm&quot;), legend.text = element_text(size = 10)), guides(shape = guide_legend(override.aes = list(size = 5)), color = guide_legend(override.aes = list(size = 5))) ) ggplot(data = df.hemi %&gt;% filter(mriloc2 == &quot;CingC&quot;, metric == &quot;MD&quot;)) + aes(x = hemisphere, y = dtiChange) + gglayers Below is another example using a loop, which is a common task you might need to accomplish # Making a loop to plot several items -------- # I have an example of this in JumpCut projects # Example 1 ===================== #Now this will be for(i in 1:length(ERP_Component)) &amp;&amp; each electNum electName &lt;- unique(erp$electNum) erpComp &lt;- unique(erp$ERP_component) cond &lt;- unique(erp$Condition) cond &lt;- cond[sort.list(cond)] for(ii in 1:(length(erpComp))) { for(xx in 1:(length(electName))) { tmp_erp &lt;- subset(erp, ERP_component == erpComp[ii] &amp; electNum == electName[xx]) #tmp_erp &lt;- subset(erp, ERP_component == &quot;P3b_Lat&quot; &amp; electNum == &quot;E015&quot;) #Now get each condition lay = rbind(c(1,2), c(3,4)) #sets up the layout of my arranged plots plot_list = list() #p1 &lt;- ggplot(data.frame(tmp2), aes(x=AthleteType), fill = AthleteType) + xlab(&quot;Athlete Type&quot;) for(i in 1:(length(cond))) { tmp2 &lt;- subset(tmp_erp, Condition == cond[i]) #Creates a subset that has only GoC data when i=1 tmp3 &lt;- tmp2 %&gt;% group_by(Timet, AthleteType) %&gt;% summarise(mean=ci(ERP_value)[1], lowCI=ci(ERP_value)[2], hiCI=ci(ERP_value)[3], sd=ci(ERP_value)[4]) p1 &lt;- ggplot(tmp3, aes_string(x=names(tmp3)[2],y=names(tmp3)[3],fill=names(tmp3)[2])) #dl[2] = Group p1 &lt;- p1 + geom_bar(stat=&quot;identity&quot;) + geom_errorbar(aes(ymin=lowCI, ymax=hiCI),width=.2) + facet_wrap(~Timet) + scale_fill_grey() + theme(legend.position=&quot;none&quot;, axis.text=element_text(size=8),axis.title.x=element_blank(), axis.title.y=element_blank(),plot.title = element_text(hjust = 0.5)) p1 &lt;- p1 + geom_jitter(data=tmp2,mapping= aes_string(x=names(tmp2)[10], y = names(tmp2)[8], color = names(tmp2)[1]),position=position_jitter(width=.25, height=0),size = 2.9, alpha = 9/10) + theme(legend.position=&quot;none&quot;) if (i==1){ p1 &lt;- p1 + labs(title = &quot;Correct/Go Condition&quot;) } else if (i==2) { p1 &lt;- p1 + labs(title = &quot;Correct/No-Go Condition&quot;) } else if (i==3) { p1 &lt;- p1 + labs(title = &quot;Incorrect/No-Go Condition&quot;) } plot_list[[i]] &lt;- p1 } # Let&#39;s create our y axis label tmp_lbl &lt;- unique(tmp2$ERP_component) underscoreLocation &lt;- unlist(gregexpr(pattern =&#39;_&#39;,tmp_lbl)) #Gather the location of the underscore needed to create the labels if (endsWith(tmp_lbl,&quot;Lat&quot;) ==1){ #Here I need to get the ERP Component Name. I had this in a loop but because of the different sizes (N2 = 2 vs ERN = 3 characters) tmp_CompName = substr(tmp_lbl,start=1,stop=underscoreLocation-1) tmp_lbl2 &lt;- paste(electName[i],tmp_CompName,&quot;Local Peak Latency (ms)&quot;) p &lt;- grid.arrange(plot_list[[1]],plot_list[[2]],plot_list[[3]], layout_matrix = lay, bottom = textGrob(&quot;Note: Different colors in the jitter represent different athletes&quot;, gp=gpar(fontsize=12,font=3), hjust=0.1), left = textGrob(paste(tmp_lbl2), rot = 90, vjust = 1)) #p dir_lbl = paste0(&quot;images/ERP_GroupBar/PeakLat/&quot;,tmp_CompName,&quot;PeakLat_&quot;,unique(tmp2$electNum),&quot;.png&quot;) #using paste0 removes the spaces ggsave(dir_lbl,p, width=11, height=8.5, dpi=400) }else if (endsWith(tmp_lbl,&quot;Amp&quot;) ==1){ tmp_CompName = substr(tmp_lbl,start=1,stop=underscoreLocation-1) tmp_lbl2 &lt;- paste(tmp_CompName,&quot;Local Peak Amplitude (&quot;) p &lt;- grid.arrange(plot_list[[1]],plot_list[[2]],plot_list[[3]], layout_matrix = lay, bottom = textGrob(&quot;Note: Different colors in the jitter represent different athletes&quot;, gp=gpar(fontsize=12,font=3), hjust=0.1), left = textGrob(bquote(.(tmp_lbl2)*mu*.(&quot;V)&quot;)), rot = 90, vjust = 1)) #p dir_lbl = paste0(&quot;images/ERP_GroupBar/PeakAmp/&quot;,tmp_CompName,&quot;PeakAmp_&quot;,unique(tmp2$electNum),&quot;.png&quot;) #using paste0 removes the spaces ggsave(dir_lbl,p, width=11, height=8.5, dpi=400) } } } rm(underscoreLocation, tmp_CompName, tmp_lbl,tmp_lbl2, dir_lbl, i, ii, xx, electName, erpComp, cond, tmp3, tmp2, lay) # Example 2 ============== #Create a string with the y-labels par(ask=TRUE) out &lt;- NULL p &lt;- ggplot(data.frame(dl), aes(x=dl$Groupt), fill = Groupt) + xlab(&quot;Groups&quot;) for(i in 10:(ncol(dl)-0)) { p &lt;- ggplot(dl, aes_string(x=names(dl)[2], y = names(dl)[i], fill = names(dl)[2])) p &lt;- p + stat_summary(fun.y = mean, geom = &quot;bar&quot;) + stat_summary(fun.data = mean_cl_normal, geom = &quot;errorbar&quot;, width = 0.2) + facet_wrap(~CondBt) + theme(legend.position=&quot;none&quot;, axis.title.x=element_blank()) print(p) out[[i-9]] &lt;- p } s1 &lt;- marrangeGrob(grobs=out, nrow = 2, ncol=2) ggsave(&quot;try2.pdf&quot;,s1) dev.off "],["basic-statistics.html", "Chapter 8 Basic Statistics 8.1 Preamble 8.2 Running your models 8.3 Modeling your data 8.4 Statistics - Resources 8.5 Looking at interactions ", " Chapter 8 Basic Statistics 8.1 Preamble This could very well be a book on its own. We are not going to go into too much detail here, but I will show a few statistics you can run for exploratory measures. Please understand that these stats should NOT be what is included in your final manuscript. They are simply a quick and dirty way to gain an understanding of your data. 8.2 Running your models For starters I am a big fan of using something that allows me to run through several models to get an overall feel for the data. This usually involves either broom or rstatix package which allows for pipe friendly modelling. I will commonly do this in order to evaluate if random factors should be included in my final model (covered in the mixed model chapter). # Question: Is there a subject effect in our data? # if there is then we introduce it as a random effect. tbl.SubjectEffect &lt;- df.nirs %&gt;% group_by(metric) %&gt;% do(tidy(lm(value ~ Subject, .))) # requires broom tbl.SubjectEffect &lt;- df.nirs %&gt;% group_by(metric) %&gt;% rstatix::anova_test(value~Subject) # answer is yes. There is a subject effect that we must accomodate For a very complete guide on running ANOVAs in R for your first time. This post is immaculate. You can see some exercises here 8.3 Modeling your data To test the normality of your data you can use a few different methods http://www.sthda.com/english/wiki/normality-test-in-r https://rstudio-pubs-static.s3.amazonaws.com/2002_1f803b2bc84c46008d3599a07867a95a.html https://www.datanovia.com/en/lessons/anova-in-r/ Plot your data Check skewness and kurtosis Shapiro test. In order to visualize your linear model I use 3 packages/BlandAltmanLeh/vignettes/Intro jtools interactions ggeffects (used less) To visualize lmer mixed models see here To follow up a mixed-model here If you want a very quick way to look at your data, using rstatix is my #1 recommendation. ggstatsplot is a nice package that gets around a few problems you may run into. The broom package for visualising models 8.4 Statistics - Resources 8.4.1 Correlations in R/random_slopes http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5383908/ https://cran.r-project.org/web/packages/sjPlot/vignettes/sjtitemanalysis.html 8.4.2 Correlation Plots: http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram 8.4.3 General Good Sites https://statisticsbyjim.com/jim_frost/ 8.4.4 Learn about Residuals: http://docs.statwing.com/interpreting-residual-plots-to-improve-your-regression/ https://statisticsbyjim.com/regression/check-residual-plots-regression-analysis/ https://rpubs.com/iabrady/residual-analysis 8.4.5 Regression Diagnostics https://www.statmethods.net/stats/rdiagnostics.html tidymodels : https://www.tidymodels.org/ 8.4.6 Bland-Altman Plots https://cran.r-project.org/web/packages/BlandAltmanLeh/vignettes/Intro.html 8.4.7 ANOVA https://www.datanovia.com/en/lessons/anova-in-r/ 8.4.8 Mixed Models Good introduction: https://ourcodingclub.github.io/tutorials/mixed-models/ More intro: https://www.jaredknowles.com/journal/2013/11/25/getting-started-with-mixed-effect-models-in-r https://dynamicecology.wordpress.com/2015/02/05/how-many-terms-in-your-model-before-statistical-machismo/ https://m-clark.github.io/mixed-models-with-R/random_slopes.html Package that gives pseudo-R^2 for mixed models &gt; https://cran.r-project.org/web/packages/jtools/vignettes/summ.html#report_robust_standard_errors https://rinterested.github.io/statistics/mixed_effects_comparison.html 8.4.9 Cluster Analysis (see bottom of the page for more links) https://www.datanovia.com/en/product/practical-guide-to-cluster-analysis-in-r/ 8.5 Looking at interactions  src: https://cran.r-project.org/web/packages/emmeans/vignettes/interactions.html 8.5.1 Packages mertool is a gui for modelling &gt; https://www.jaredknowles.com/journal/2015/8/12/announcing-mertools tidymodels: https://www.tidymodels.org/learn/statistics/tidy-analysis/ To assess model performance I use the performance package. You can visualize your model using this vignette # Running an ANOVA ---------------- # testing to see if the type of service has an effect on the Journey Time #model &lt;- lm(journey_time_avg ~ service, data=df) model &lt;- aov(journey_time_avg ~ service, data=df) modsum &lt;- summary(model) #anova(model) #show me the answer in an ANOVA table modsum # ANOVA w/ Type III SS -------------- # requires the &quot;car&quot; library model &lt;- Anova(lm(Measure ~ Group + Brain_Region + Group*Brain_Region, data = df, contrasts=list(Group=contr.sum, Brain_Region=contr.sum), type=3)) report(model) #requires easystats describe.aov(model, &#39;Group:Brain_Region&#39;, sstype = 2) #requires apastats options(contrasts = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;)) Anova(model, type=&quot;III&quot;) # Type III tests # Post-Hoc according to Field&#39;s Book --------- contrasts(df$Group)=contr.poly(3) #model.1=aov(dv~covariate+factorvariable, data=dataname) model.1=aov(Measure ~ Baseline + Group + Brain_Region + Group*Brain_Region, data = df) Anova(model.1, type=&quot;III&quot;) # Make sure you use capital &quot;A&quot; Anova here and not anova. This will give results using type III SS. posthoc=glht(model.1, linfct=mcp(Group=&quot;Tukey&quot;)) ##gives the post-hoc Tukey analysis summary(posthoc) ##shows the output in a nice format. confint(posthoc) # Give me the confidence intervals for the post-hoc tests effect(&quot;Group&quot;, model.1) # Give me the group effects Below is another loop # Run Several models using sapply -------------------- # IVs and covariates ivs &lt;- c(&quot;Neurological_Infection&quot;) covariates &lt;- c(&quot;Etiology_of_Injury&quot;, &quot;Open_Closed_Injury&quot;, &quot;SDH&quot;, &quot;EDH&quot;, &quot;Intracranial_monitors&quot;, &quot;Non_neurologic_surgery&quot;, &quot;Skull_fracture&quot;, &quot;ICU_LOS_in_days&quot;, &quot;Time_to_surgery&quot;, &quot;Site_of_surgery&quot;, &quot;Craniotomy&quot;, &quot;Craniotomy_OR_duration&quot;, &quot;Craniectomy&quot;, &quot;Craniectomy_OR_duration&quot;, &quot;Bone_removal_surgeries&quot;, &quot;Initial_Cranioplasty&quot;, &quot;Days_to_initial_cranioplasty&quot;, &quot;Initial_Cranioplasty_OR_Duration&quot;, &quot;Initial_Cranioplasty_bone_material&quot;, &quot;Bone_replacement_surgeries&quot;, &quot;Duraplasty_material&quot;, &quot;Other_Infection&quot;, &quot;Non_periop_antibiotics&quot;, &quot;Time_of_day_of_initial_surgery&quot;, &quot;Week_weekend_of_initial_surgery&quot;, &quot;Hydrocephalus&quot;, &quot;Post_op_complications&quot;, &quot;EtOH_substance_abuse&quot;, &quot;Anticoagulation&quot;, &quot;Seizure&quot;, &quot;HTN&quot;, &quot;Smoker&quot;, &quot;Diabetes&quot;, &quot;Heart_disease&quot;, &quot;Liver_disease&quot;, &quot;Kidney_disease&quot;, &quot;Previous_brain_injury_surgery&quot;) univ_formulas &lt;- sapply(covariates, function(x) as.formula(paste(&#39;Neurological_Infection~&#39;, x))) univ_models &lt;- lapply( univ_formulas, function(x){glm(x, ,family=binomial(link=&#39;logit&#39;),data=df1)}) # Making a loop to run several stats and saving them as a table -------- # I have an example of this in JumpCut projects cond = levels(erp$Condition) comp = levels(erp$ERP_component) erp$NumUndiagConc &lt;- as.factor(erp$NumUndiagConc) erp$NumPriorConc &lt;- as.factor(erp$NumPriorConc) res &lt;- data.frame() for (yy in (1:(length(cond)))) { data &lt;- subset(erp, Condition == cond[yy]) if ((cond[yy]==&quot;rGoC&quot;) | (cond[yy]==&quot;rNgW&quot;)){ comp &lt;- c(&quot;ERN_Amp&quot;,&quot;ERN_Lat&quot;,&quot;Pe_Amp&quot;,&quot;Pe_Lat&quot;) } else if ((cond[yy]!=&quot;rGoC&quot;) | (cond[yy]!=&quot;rNgW&quot;)){ comp = c( &quot;N2_Amp&quot;, &quot;N2_Lat&quot;, &quot;P3a_Amp&quot;,&quot;P3a_Lat&quot;, &quot;P3b_Amp&quot;, &quot;P3b_Lat&quot;) } res1 &lt;- data.frame() #reset after each condition loop for (zz in (1:(length(comp)))) { compData &lt;- subset(data,ERP_component == comp[zz]) electNum = compData$electNum if (length(unique(electNum))==1){ fm1 &lt;- lmer(ERP_value~AthleteType + Time + AthleteType*Time + (1|id) ,data=compData) } else if (length(unique(electNum))&gt;=1){ fm1 &lt;- lmer(ERP_value~AthleteType + NumPriorConc+ AthleteType*Time +AthleteType*NumPriorConc*Time + (1|id) + (1|electNum),data=compData) } assign(paste(&quot;mod&quot;, cond[yy],gsub(&quot;_&quot;,&quot;&quot;,comp[zz]), sep=&quot;_&quot;), fm1) #save it in the form of modsum_GoC_N2Amp assign(paste(&quot;modsum&quot;, cond[yy],gsub(&quot;_&quot;,&quot;&quot;,comp[zz]), sep=&quot;_&quot;), summary(fm1)) #save it in the form of modsum_GoC_N2Amp tmod &lt;- as.data.frame(summary(fm1)$coefficients) tmod &lt;- round(tmod[-c(1), ],3) tmod &lt;- subset(tmod,`Pr(&gt;|t|)`&lt;= 0.1) if (nrow(tmod)==0){ a &lt;- 1 } else if (nrow(tmod)&gt;=0){ Test &lt;- rownames(tmod) tmod &lt;- cbind(Test,tmod) rownames(tmod) &lt;- NULL #Component &lt;- c(rep_len(paste(cond[yy],&#39; &#39;,comp[zz]),length.out = length(Test)))#comp[zz] Component &lt;- c(rep_len(paste(comp[zz]),length.out = length(Test)))#comp[zz] tmod &lt;- cbind(Component,tmod) res &lt;- rbind(res,tmod) tmod$`Pr(&gt;|t|)`[ tmod$`Pr(&gt;|t|)` &lt;0.001] &lt;- &quot;&lt;0.001&quot; tmod$`Pr(&gt;|t|)` &lt;- sub(&quot;[0].&quot;, &quot;.&quot;, tmod$`Pr(&gt;|t|)`) #tmod$`t value` &lt;- round(tmod$`t value`,digits=2) tmod$Result &lt;- paste0(&quot;t(&quot;, round(tmod$df,0), &quot;)=&quot;, sprintf(&quot;%.2f&quot;, tmod$`t value`), &quot;, p=&quot;, tmod$`Pr(&gt;|t|)`) tmod &lt;- subset(tmod,select=c(&quot;Component&quot;, &quot;Test&quot;,&quot;Result&quot;)) tmod$Test &lt;- gsub(&quot;AthleteType.L&quot;,&quot;Athlete Type&quot;,tmod$Test) tmod$Test &lt;- gsub(&quot;:&quot;,&quot;$\\\\times$&quot;, tmod$Test, fixed=TRUE) tmod$Component &lt;- gsub(&quot;_&quot;,&quot; &quot;,tmod$Component) tmod$Test &lt;- gsub(&quot;Time&quot;,&quot;Time &quot;,tmod$Test) res1 &lt;- rbind(res1,tmod) } assign(paste0(cond[yy],&quot;statsTable&quot;), res1) #save it in the form of modsum_GoC_N2Amp } } "],["mixed-models-1.html", "Chapter 9 Mixed Models 9.1 To-Do 9.2 Preamble 9.3 Post-hoc in mixed models", " Chapter 9 Mixed Models 9.1 To-Do Look at the presentation (Google Slides) I made for Keiths lab (the one Ashley asked me to make). It has a ton of information I can use here. 9.2 Preamble I made mixed models its own section because it is not your typical analyses. I would highly recommend that any student starts with more traditional analyses such as ANOVA or linear regression before taking on mixed model analyses. # to speed up computation, let&#39;s use only 50% of the data set.seed(123) # linear model (model summaries across grouping combinations) broomExtra::grouped_glance( data = dplyr::sample_frac(tbl = ggplot2::diamonds, size = 0.5), grouping.vars = c(cut, color), formula = price ~ carat - 1, ..f = stats::lm, na.action = na.omit ) # linear mixed effects model (model summaries across grouping combinations) broomExtra::grouped_glance( data = dplyr::sample_frac(tbl = ggplot2::diamonds, size = 0.5), grouping.vars = cut, ..f = lme4::lmer,4 formula = price ~ carat + (carat | color) - 1, control = lme4::lmerControl(optimizer = &quot;bobyqa&quot;) fm1 &lt;- lmer(Reaction ~ Days + (Days | Subject), sleepstudy) # df.residual in `glance` broom.mixed::tidy(fm1) broom.mixed::glance(fm1) # fetch the p-values parameters::model_parameters(fm1) %&gt;% broomExtra::easystats_to_tidy_names() ) # p-values across groups in mixed-model iris %&gt;% group_by(Species) %&gt;% group_modify(., ~ broomExtra::easystats_to_tidy_names(as.data.frame(parameters::model_parameters( lme4::lmer(Sepal.Length ~ Petal.Length + Petal.Width + Petal.Length * Petal.Width + (1 | Sepal.Width), data = .x, control = lme4::lmerControl(optimizer = &quot;bobyqa&quot;) ) )))) Here is code I used from a recent project tmp &lt;- clear.labels(df.nirs) %&gt;% group_by(metric) %&gt;% group_modify(., ~ broomExtra::easystats_to_tidy_names(as.data.frame(parameters::model_parameters( lme4::lmer(value ~ Connection*Task + group*Connection + group*Task + group*Connection*Task + (1|Subject), data = .x, control = lme4::lmerControl(optimizer = &quot;bobyqa&quot;) ) )))) %&gt;% filter(term != &quot;(Intercept)&quot;) %&gt;% # remove intercept from report mutate(term = str_replace_all(term, &quot;:&quot;, &quot;×&quot;) %&gt;% str_replace_all(&quot;Connection&quot;, &quot;&quot;) %&gt;% str_replace_all(&quot;Task&quot;, &quot;&quot;) %&gt;% str_replace_all(&quot;group.L&quot;, &quot;Group&quot;) %&gt;% str_replace_all(&quot;-&gt;&quot;, &quot;&quot;) ) %&gt;% janitor::clean_names(&quot;upper_camel&quot;) %&gt;% dplyr::rename( &quot;p&quot; = &quot;PValue&quot;, &quot;dferror&quot; = &quot;DfError&quot; ) %&gt;% filter((str_detect(Term, &quot;Group&quot;)) &amp; p &lt; .1) %&gt;% # Give me only the results that show group differences, set p @.1 mutate_if(is.numeric, round, 5) # round to 5 digits (makes it easier to read) View(tmp) To visualize your mixed model you can use sjPlot as shown below or here pacman::p_load(sjPlot, sjmisc, ggplot2) data(efc) theme_set(theme_sjplot()) # make categorical efc$c161sex &lt;- to_factor(efc$c161sex) # fit model with interaction fit &lt;- lm(neg_c_7 ~ c12hour + barthtot * c161sex, data = efc) plot_model(fit, type = &quot;pred&quot;, terms = c(&quot;barthtot&quot;, &quot;c161sex&quot;)) plot_model(fit, type = &quot;int&quot;) plot_model(fit, type = &quot;pred&quot;, terms = c(&quot;c161sex&quot;, &quot;barthtot [0, 100]&quot;)) fit &lt;- lm(neg_c_7 ~ c12hour + c161sex * barthtot, data = efc) plot_model(fit, type = &quot;int&quot;) plot_model(fit, type = &quot;int&quot;, mdrt.values = &quot;meansd&quot;) # fit model with 3-way-interaction fit &lt;- lm(neg_c_7 ~ c12hour * barthtot * c161sex, data = efc) # select only levels 30, 50 and 70 from continuous variable Barthel-Index plot_model(fit, type = &quot;pred&quot;, terms = c(&quot;c12hour&quot;, &quot;barthtot [30,50,70]&quot;, &quot;c161sex&quot;)) plot_model(fit, type = &quot;int&quot;) 9.3 Post-hoc in mixed models This can be a bit tricky because you want to include your random effect. You could use emmeans::emmeans() lmerTest::difflsmeans() multcomp::glht() "],["running-your-models-1.html", "Chapter 10 Running your models", " Chapter 10 Running your models For starters I am a big fan of using something that allows me to run through several models to get an overall feel for the data. This usually involves either broom or rstatix package which allows for pipe friendly modelling. I will commonly do this in order to evaluate if random factors should be included in my final model. If you are running mixed models you can use broomExtra or broom.mixed. I would recommend broomExtra which depends on broom.mixed (so its hits 2 birds with one stone). # Question: Is there a subject effect in our data? # if there is then we introduce it as a random effect. tbl.SubjectEffect &lt;- df.nirs %&gt;% group_by(metric) %&gt;% do(tidy(lm(value ~ Subject, .))) # requires broom tbl.SubjectEffect &lt;- df.nirs %&gt;% group_by(metric) %&gt;% rstatix::anova_test(value~Subject) # answer is yes. There is a subject effect that we must accomodate For a very complete guide on running ANOVAs in R for your first time. This post is immaculate. You can see some exercises here "],["modeling-your-data-1.html", "Chapter 11 Modeling your data 11.1 Looking at interactions 11.2 Why should I do this? 11.3 Installing Zotero 11.4 YAML Explained", " Chapter 11 Modeling your data To test the normality of your data you can use a few different methods Plot your data Check skewness and kurtosis Shapiro test. In order to visualize your linear model I use a few packages sjPlot and sjmisc jtools interactions ggeffects (used less) To visualize lmer mixed models see here To follow up a mixed-model here # Load typical packages pacman::p_load(sjPlot, sjmisc, ggplot2, lme4) model &lt;- lm(mpg ~ wt + cyl + gear + disp, data = mtcars) r &lt;- report::report(model) table_short(r) performance::check_model(Model) # will make some plots to check your model # Individually checking model. # Note: I will do this if I see something problematic after running `check_model` parameters::model_parameters(model) performance::check_collinearity(model) performance::plot(check_collinearity(model)) I am still working on code that will do this when I run my mixed models in a loop. But the GitHub page should help # Load typical packages pacman::p_load(sjPlot, sjmisc, ggplot2, lme4) model &lt;- lm(mpg ~ wt + cyl + gear + disp, data = mtcars) r &lt;- report::report(model) table_short(r) performance::check_model(Model) # will make some plots to check your model # Individually checking model. # Note: I will do this if I see something problematic after running `check_model` parameters::model_parameters(model) performance::check_collinearity(model) performance::plot(check_collinearity(model)) 11.0.1 Mixed Models: Misc Info Contrast Analysis post-hoc mixed models Visualize your mixed models Good introduction: https://ourcodingclub.github.io/tutorials/mixed-models/ More intro: https://www.jaredknowles.com/journal/2013/11/25/getting-started-with-mixed-effect-models-in-r https://dynamicecology.wordpress.com/2015/02/05/how-many-terms-in-your-model-before-statistical-machismo/ https://m-clark.github.io/mixed-models-with-R/random_slopes.html Package that gives pseudo-R^2 for mixed models https://cran.r-project.org/web/packages/jtools/vignettes/summ.html#report_robust_standard_errors https://rinterested.github.io/statistics/mixed_effects_comparison.html easystats blog. Has some great posts with code included. 11.1 Looking at interactions src: https://cran.r-project.org/web/packages/emmeans/vignettes/interactions.html 11.1.1 Packages mertool is a gui for modelling &gt; https://www.jaredknowles.com/journal/2015/8/12/announcing-mertools tidymodels: https://www.tidymodels.org/learn/statistics/tidy-analysis/ To assess model performance I use the performance package. You can visualize your model using this vignette Making your first Reproducible Document 11.2 Why should I do this? The easiest answer is TIME. In my experience when you finish a project 11.3 Installing Zotero If you do plan on being able to write documents then I suggest using Zotero. 11.4 YAML Explained YAML is the first part of code in your *.Rmd document. It controls how your document will be compiled. ## "],["example-reproducible-document.html", "Chapter 12 Example Reproducible Document 12.1 Results", " Chapter 12 Example Reproducible Document 12.1 Results There was a significant effect of service on the average journey time 12.1.1 Referencing a table Descriptive statistics were run and are shown in Table ?? below knitr::kable(iris, longtable=T, caption = &quot;Descriptives&quot;) 12.1.2 Referencing a Figure knitr::include_graphics(&quot;images/plot1.png&quot;) A plot is shown in Figure ?? "],["references.html", "References 12.2 Useful Links 12.3 Installing Zotero 12.4 YAML Explained", " References Working with Time Series Data The first think you will need to do is convert you data.frame into a time series timeseries &lt;- ts(myDataFrame) 12.2 Useful Links https://a-little-book-of-r-for-time-series.readthedocs.io/en/latest/src/timeseries.html https://rstudio-pubs-static.s3.amazonaws.com/84226_ad792383c050483bbae4676bc76a4038.html RcppRoll package seems very useful. 12.3 Installing Zotero If you do plan on being able to write documents then I suggest using Zotero. 12.4 YAML Explained YAML is the first part of code in your *.Rmd document. It controls how your document will be compiled. ## "],["example-reproducible-document-1.html", "Chapter 13 Example Reproducible Document 13.1 Results", " Chapter 13 Example Reproducible Document 13.1 Results There was a significant effect of service on the average journey time 13.1.1 Referencing a table Descriptive statistics were run and are shown in Table ?? below knitr::kable(iris, longtable=T, caption = &quot;Descriptives&quot;) 13.1.2 Referencing a Figure knitr::include_graphics(&quot;images/plot1.png&quot;) A plot is shown in Figure ?? "],["how-to-create-a-github-key.html", "Chapter 14 How to create a GitHub key", " Chapter 14 How to create a GitHub key You are going to be installing packages from multiple sources. When installing a package for the first time you might need to install it from GitHub. Its a great source for packages that have not yet been released on CRAN. Under normal circumstances your ability to install packages from GitHub is limited. However you can create an authentication key which will give you unlimited power. I would highly suggest doing this during your setup. "]]
